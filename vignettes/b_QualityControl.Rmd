---
title: "2. Filtering Data"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{2. Filtering Data}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(beastr)
```


## Motivation

_What is "bad" data?_

Before we analyze data, we need to make sure it's not bogus. With GPS telemetry
we get a lot of questionable data. 

All of the data recorded is useful, so we won't
remove it outright. For location data, the most common type of bad data is a 
missed fix. This is where the collar did not find enough satellites to obtain a
location. This data, however, is still useful. The inability to find satellites
is not random. It is associated terrain features. It is more likely to find 
signals on the top of a mountain than the bottom of a canyon. If we were to 
analyze an animal's habitat use without taking these missed fixes into account, 
it would be biased towards the tops of ridges and peaks. 

Beside large scale terrain features, this missed data can give us information 
about microhabitat and animal behavior.

Here's an example:

```{r fig.width=6, message=FALSE, warning=FALSE}
library(ggplot2)
library(dplyr)

# example database:
dsn = system.file("db/telemetry.gpkg", package = "beastr")

# first make sure the example data is in the example database:

# Recursing a directory to find lotek fix files:
fix_files = fs::dir_ls(system.file("lotek/", package = "beastr"),
                       regexp = "[/\\]PinPoint[ 0-9-]*.txt$",
                       recurse = TRUE)

# Add those fixes to database:
append_database(dsn = dsn, 
                fix_files = fix_files)

# Get points:
get_animal_fixes(dsn) %>%
  ggplot(aes(x=temp_c)) +
  geom_density(aes(fill = fix_status ), binwidth = 1, alpha = 0.7) +
  ggtitle("Fix Success vs Temp")

```

In this plot, we can see that the missed fixes are highly associated with the 
collar temperature. This indicates that the animals are resting in spots with 
poor satellite view. (Higher temperatures indicate the sensor is reading the 
animal's body rather than the air.)

Given that caveat about bad data, missed fixes don't actually have explicity 
spatial coordinates associated with them. So we will remove them for mapping. 

```{r fig.height=6, fig.width=8, message=FALSE, warning=FALSE}
library(mapview) 
library(leaflet)
library(leaflet.providers)

get_animal_fixes(dsn) %>% 
  filter(fix_status == "Valid") %>% 
  mapview(zcol = "animal_id") -> 
  m

m

```

You may notice some outlying points. Obviously we have fixes with actual 
coordinates that still aren't good. How can we remove these?

## Filters

There are quite a few different measures of how bad a fix is. Some of these come 
with the data, and some we have to compute ourselves. The most reliable approach 
is to use multiple measures and filter the data based on thresholds. Picking these
thresholds is somewhat mystical. Let's take a look at some of them:

### Dilution of Precision

Dilution of Precision (DOP) measures are calculated by the GPS receiver based on
satellite precision. There are multiple variants. Common ones are horizontal and
vertical. THe closer together the satellites are in the sky, the les 

Lotek gives us horizontal dilution of precision (hdop).  Let's take a 
look at the distribution from our example dB:

```{r fig.height=6, fig.width=8, message=FALSE, warning=FALSE}
get_animal_fixes(dsn) %>% 
  ggplot(aes(x = hdop)) +
  geom_histogram(binwidth = 0.1) +
  xlab("hdop, (log scale)") +
  scale_x_log10()
```

Here we can see that most of the HDOP values are under 10. The ones above 10 are likely to have a high error. 
We obviously don't have a way to directly measure error for each of these points, but we can get an indication that hdop is associated with error by looking at displacement. In this case we can define displacement at the distance from the mean point. If high HDOP points tend to have high displacement, that's a good indication that it is associated with error. Let's take a look:

```{r message=FALSE, warning=FALSE}
# load sf for spatial processing:
library(sf)

# calculate mean points per animal:
get_animal_fixes(dsn) %>% 
  group_by(animal_id) %>% 
  summarise(geom = sf::st_union(geom)) %>% 
  mutate(centroid = st_centroid(geom)) %>% 
  st_drop_geometry() ->
  animal_centroids

# calculate displacement for each point
get_animal_fixes(dsn) %>% 
  filter(fix_status == "Valid") %>% 
  left_join(animal_centroids) %>% 
  mutate(disp = st_distance(geom, centroid, by_element = TRUE)) ->
  fixes_w_displacement

# log scales:
fixes_w_displacement %>% 
  mutate(disp = as.numeric(disp)) %>% 
  ggplot(aes(x = disp, y = hdop)) + 
  geom_point(alpha = 0.5) + 
  scale_x_log10() + 
  scale_y_log10()
```

This is interesting because we see a natural boundary in displaement. This is at roughly 8km, so that makes sense for an animal (fisher in this case) staying within a typical home range. There are two distinct features of this plot to notice: 

  - Nearly all the points with displacement outside the typical homerange have high HDOP. These are almost certainly bad points. Points that we could probably tell are bad just from the map. 
  - There are, however, still many points within the typical displacements that have similarly high HDOP. These are the reason we need these data filters beyond just looking at the map and throwing away points that look absurd. These points might look normal on a map but are still probably bad. 
  
  
From this plot, I would use the points in the upper right sector to pick an ad-hoc cutoff for HDOP. Probably ~100. It's better to be conservative and not throw away too many points. That's why we'll apply multiple filters to narrow down other bad fixes.
